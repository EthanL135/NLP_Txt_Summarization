# -*- coding: utf-8 -*-
"""NLP Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zBx_e0na4dat3li6vlPn7QyDHp8KFBrV
"""

# Installs required libraries
!pip install torch torchvision transformers datasets evaluate bert-score nltk matplotlib seaborn

# Imports libraries
import torch
from datasets import load_dataset, DatasetDict
from evaluate import load
from transformers import BartForConditionalGeneration, BartTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer
from bert_score import score
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML
import numpy as np
nltk.download("punkt")

# Loads and splits the dataset
ds = load_dataset("d0rj/wikisum")

# Tokenizes the dataset
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")

def tokenize_function(examples):
    model_inputs = tokenizer(
        examples["article"],
        max_length=512,
        truncation=True,
        padding="max_length",
    )
    labels = tokenizer(
        examples["summary"],
        max_length=128,
        truncation=True,
        padding="max_length",
    )["input_ids"]
    labels = [
        [(label if label != tokenizer.pad_token_id else -100) for label in label_list]
        for label_list in labels
    ]
    model_inputs["labels"] = labels
    return model_inputs

tokenized_datasets = ds.map(tokenize_function, batched=True)

# Loads the pre-trained BART model
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn").to("cuda")

# Sets training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=16,
    gradient_accumulation_steps=2,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    fp16=True,
    dataloader_num_workers=8,
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=100,
    save_strategy="epoch",
    load_best_model_at_end=True,
    save_total_limit=1,
)

rouge = load("rouge")
bert_scores_across_epochs = []
rouge_scores_across_epochs = []

# Defines evaluation and metrics
final_decoded_preds = []
final_decoded_labels = []

def evaluate_after_epoch(eval_preds):
    global final_decoded_preds, final_decoded_labels

    predictions, labels = eval_preds
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    final_decoded_preds = decoded_preds
    final_decoded_labels = decoded_labels
    rouge_results = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    subset_size = min(len(decoded_preds), 500)
    P, R, F1 = score(decoded_preds[:subset_size], decoded_labels[:subset_size], lang="en")
    bert_score_results = {"precision": P.mean().item(), "recall": R.mean().item(), "f1": F1.mean().item()}
    torch.cuda.empty_cache()
    rouge_scores_across_epochs.append(rouge_results)
    bert_scores_across_epochs.append(bert_score_results)

    return {
        "rouge1": rouge_results["rouge1"],
        "rouge2": rouge_results["rouge2"],
        "rougeL": rouge_results["rougeL"],
        "rougeLsum": rouge_results["rougeLsum"],
        "bert_precision": bert_score_results["precision"],
        "bert_recall": bert_score_results["recall"],
        "bert_f1": bert_score_results["f1"],
    }

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    compute_metrics=evaluate_after_epoch,
)

trainer.train()

# Saves the model after training
model.save_pretrained("./fine_tuned_bart")
tokenizer.save_pretrained("./fine_tuned_bart")

print("Model saved to './fine_tuned_bart'")

# Plots ROUGE scores across epochs
epochs = list(range(1, len(rouge_scores_across_epochs) + 1))
rouge1_scores = [score["rouge1"] for score in rouge_scores_across_epochs]
rouge2_scores = [score["rouge2"] for score in rouge_scores_across_epochs]
rougeL_scores = [score["rougeL"] for score in rouge_scores_across_epochs]

plt.figure(figsize=(10, 5))
plt.plot(epochs, rouge1_scores, label="ROUGE-1", marker='o')
plt.plot(epochs, rouge2_scores, label="ROUGE-2", marker='o')
plt.plot(epochs, rougeL_scores, label="ROUGE-L", marker='o')
plt.xlabel("Epoch")
plt.ylabel("ROUGE Score")
plt.legend()
plt.title("ROUGE Scores Across Epochs")
plt.show()

# Highlights the token-level overlap
def highlight_overlap(prediction, reference):
    pred_tokens = set(prediction.split())
    ref_tokens = set(reference.split())
    overlap = pred_tokens & ref_tokens

    ref_highlighted = " ".join([f"<b>{word}</b>" if word in overlap else word for word in reference.split()])
    pred_highlighted = " ".join([f"<b>{word}</b>" if word in overlap else word for word in prediction.split()])

    display(HTML(f"<h3>Reference Summary:</h3><p>{ref_highlighted}</p>"))
    display(HTML(f"<h3>Generated Summary:</h3><p>{pred_highlighted}</p>"))

highlight_overlap(final_decoded_preds[0], final_decoded_labels[0])

# Plots length distributions for reference and generated summaries
ref_lengths = [len(ref.split()) for ref in final_decoded_labels]
gen_lengths = [len(pred.split()) for pred in final_decoded_preds]

plt.figure(figsize=(10, 5))
plt.hist(ref_lengths, bins=20, alpha=0.7, label="Reference Summary Lengths", color="blue")
plt.hist(gen_lengths, bins=20, alpha=0.7, label="Generated Summary Lengths", color="orange")
plt.xlabel("Length (number of words)")
plt.ylabel("Frequency")
plt.legend()
plt.title("Length Distributions of Summaries")
plt.show()

# Load the model if needed
"""
model = BartForConditionalGeneration.from_pretrained("./fine_tuned_bart").to("cuda")
tokenizer = BartTokenizer.from_pretrained("./fine_tuned_bart")

print("Model loaded")
"""

# Testing on the test set
test_results = trainer.evaluate(tokenized_datasets["test"])
print("Test Results:", test_results)

# Generates predictions on the test set
test_predictions = []
test_references = []
batch_size = 16
test_dataset = tokenized_datasets["test"]

for i in range(0, len(test_dataset), batch_size):
    inputs_batch = test_dataset[i:i+batch_size]["article"]
    references_batch = test_dataset[i:i+batch_size]["summary"]
    inputs = tokenizer(
        inputs_batch, return_tensors="pt", truncation=True, max_length=512, padding="max_length").to("cuda")
    with torch.no_grad():
        generated_ids = model.generate(inputs["input_ids"], max_length=128, num_beams=4)

    # Decodes predictions and references
    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    decoded_refs = references_batch  # No need for additional decoding as they're already in text form
    test_predictions.extend(decoded_preds)
    test_references.extend(decoded_refs)

# Displays prediction and reference for the first example in the test set
print("\nExample Prediction:")
print("Generated:", test_predictions[0])
print("Reference:", test_references[0])

# Highlights token overlap for the first example
highlight_overlap(test_predictions[0], test_references[0])

# ROUGE and token overlap for the test set
test_rouge_results = rouge.compute(predictions=test_predictions, references=test_references)
print("\nTest ROUGE Scores:", test_rouge_results)

# BERT-Score for test set
subset_size = min(len(test_predictions), 500)
P, R, F1 = score(test_predictions[:subset_size], test_references[:subset_size], lang="en")
print(f"\nBERT-Score on Test Set: Precision={P.mean().item():.4f}, Recall={R.mean().item():.4f}, F1={F1.mean().item():.4f}")
